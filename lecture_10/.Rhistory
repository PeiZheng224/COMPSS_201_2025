View(carpenter_dobkin_2009)
carpenter_dobkin_2009 %>%
ggplot(aes(x = agecell, y = all)) +
geom_point() +
geom_vline(xintercept = 21, color = "#ff0091", linewidth = 1, linetype = "dashed") +
annotate("text", x = 20.4, y = 105, label = "Minimum Drinking Age") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)") +
theme_classic(base_size = 14)
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
# You can do this in a canned routine. I am replicating what we did above (standard errors are different)
rd_out <- rdrobust(y = carpenter_dobkin_2009$ all, x = carpenter_dobkin_2009$agecell, , h=100, c = 21, p = 1, kernel = "uniform")
summary(rd_out)
# Do the same thing, but different slopes on each side. Do it by hand
lm_different_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + threshold:I(agecell - 21))
summary(lm_different_slope)
carpenter_dobkin_2009 %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
lm_quadratic <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + I((agecell -21)^2) + threshold:I(agecell - 21) +
threshold:I((agecell - 21)^2))
summary(lm_quadratic)
carpenter_dobkin_2009 %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm",
formula = y ~ x + I(x ^ 2),
se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
# Now let's limit the sample! Only folks between 20 and 22!
lm_sensitivity <- carpenter_dobkin_2009 %>%
filter(agecell >= 20 & agecell <= 22) %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + threshold:I(agecell - 21))
summary(lm_sensitivity)
carpenter_dobkin_2009 %>%
filter(agecell >= 20 & agecell <= 22) %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
set.seed(22092008) # set random number generator seed
library(pacman)
p_load(tidyverse,broom,rdrobust,estimatr,modelsummary)
# This is an excellent example from Andrew Weiss:
# hypothetical example, students take an entrance exam
# at the beginning of a school year.
# Those who score 70 or below are automatically enrolled
# in a free tutoring program and receive assistance throughout the year.
# At the end of the school year, students take a final test,
# or exit exam (with a maximum of 100 points) to measure how much
# they learned overall.
tutoring <- read_csv("tutoring_program_fuzzy.csv")
ggplot(tutoring, aes(x = entrance_exam, y = tutoring_text, color = entrance_exam <= 70)) +
# Make points small and semi-transparent since there are lots of them
geom_point(size = 1.5, alpha = 0.5,
position = position_jitter(width = 0, height = 0.25, seed = 1234)) +
# Add vertical line
geom_vline(xintercept = 70) +
# Add labels
labs(x = "Entrance exam score", y = "Participated in tutoring program") +
# Turn off the color legend, since it's redundant
guides(color = "none")
# You can generate a table showing these numbers
tutoring %>%
group_by(tutoring, entrance_exam <= 70) %>%
summarize(count = n()) %>%
group_by(tutoring) %>%
mutate(prop = count / sum(count))
ggplot(tutoring, aes(x = entrance_exam, y = exit_exam, color = tutoring)) +
geom_point(size = 1, alpha = 0.5) +
# Add a line based on a linear model for the people scoring less than 70
geom_smooth(data = filter(tutoring, entrance_exam <= 70), method = "lm") +
# Add a line based on a linear model for the people scoring 70 or more
geom_smooth(data = filter(tutoring, entrance_exam > 70), method = "lm") +
geom_vline(xintercept = 70) +
labs(x = "Entrance exam score", y = "Exit exam score", color = "Used tutoring")
tutoring_with_bins <- tutoring %>%
mutate(exam_binned = cut(entrance_exam, breaks = seq(0, 100, 5))) %>%
# Group by each of the new bins and tutoring status
group_by(exam_binned, tutoring) %>%
# Count how many people are in each test bin + used/didn't use tutoring
summarize(n = n()) %>%
# Make this summarized data wider so that there's a column for tutoring and no tutoring
pivot_wider(names_from = "tutoring", values_from = "n", values_fill = 0) %>%
rename(tutor_yes = `TRUE`, tutor_no = `FALSE`) %>%
# Find the probability of tutoring in each bin by taking
# the count of yes / count of yes + count of no
mutate(prob_tutoring = tutor_yes / (tutor_yes + tutor_no))
# Plot this puppy
ggplot(tutoring_with_bins, aes(x = exam_binned, y = prob_tutoring)) +
geom_col() +
geom_vline(xintercept = 8.5) +
labs(x = "Entrance exam score", y = "Proportion of people participating in program")
# Let's make an instrument and center this puppy
tutoring_centered <- tutoring %>%
mutate(entrance_centered = entrance_exam - 70,
below_cutoff = entrance_exam <= 70)
tutoring_centered
# Now we have a new column named below_cutoff that we’ll use as an instrument. Most of the time this will be the same as the tutoring column, since most people are compliers. But some people didn’t comply, like person 8 here who was not below the cutoff but still used the tutoring program.
# Let's pretend this cutoff is sharp.
model_sans_instrument <- lm(exit_exam ~ entrance_centered + tutoring,
data = filter(tutoring_centered,
entrance_centered >= -10 &
entrance_centered <= 10))
tidy(model_sans_instrument)
# Without realizing that this is a fuzzy design we would say this is a 11.5 point increase
# on the final because of the program. This is wrong of course. You need to run 2SLS
# Using the below cutoff as an instrument.
model_fuzzy <- iv_robust(
exit_exam ~ entrance_centered + tutoring | entrance_centered + below_cutoff,
data = filter(tutoring_centered, entrance_centered >= -10 & entrance_centered <= 10)
)
tidy(model_fuzzy)
rdrobust(y = tutoring$exit_exam, x = tutoring$entrance_exam,
c = 70, fuzzy = tutoring$tutoring) %>%
summary()
# ------------------------------------------------------------
# FE (within) vs First-Difference estimators in panel data
# ------------------------------------------------------------
# Packages
suppressPackageStartupMessages({
library(dplyr)
library(tidyr)
library(fixest)   # fast FE with cluster-robust SEs
})
install.packages("fixest")
# ------------------------------------------------------------
# FE (within) vs First-Difference estimators in panel data
# ------------------------------------------------------------
# Packages
suppressPackageStartupMessages({
library(dplyr)
library(tidyr)
library(fixest)   # fast FE with cluster-robust SEs
})
set.seed(123)
# Helper: simulate a panel with unit FE and optional AR(1) errors
sim_panel <- function(N = 500, T = 2, beta = 1.5, rho = 0, unbalanced = FALSE) {
id <- rep(1:N, each = T)
t  <- rep(1:T, times = N)
# Unit fixed effects
alpha_i <- rnorm(N, 0, 1)
alpha   <- alpha_i[id]
# Regressor with both id and time variation
# (You can tweak the signal/noise ratio as needed.)
u_x_i <- rnorm(N, 0, 1)[id]
v_x_t <- rnorm(T, 0, 1)[t]
x <- 0.7 * u_x_i + 0.3 * v_x_t + rnorm(N*T, 0, 1)
# Errors: AR(1) within unit if rho != 0, else iid
e <- numeric(N*T)
for (i in 1:N) {
idx <- which(id == i)
e_i <- rnorm(T, 0, 1)
if (rho != 0 && T > 1) {
for (k in 2:T) e_i[k] <- rho * e_i[k-1] + rnorm(1, 0, 1)
}
e[idx] <- e_i
}
y <- alpha + beta * x + e
df <- tibble(id = id, t = t, y = y, x = x)
# Optionally drop some (id, t) to create an unbalanced panel
if (unbalanced) {
df <- df %>%
group_by(id) %>%
filter(!(t == max(t) & runif(1) < 0.35)) %>%  # randomly drop last period for ~35% of units
ungroup()
}
df
}
# ------------------------------------------------------------
# 1) T = 2, iid errors  -> FE and FD should be numerically identical for beta
# ------------------------------------------------------------
df2 <- sim_panel(N = 1000, T = 2, beta = 2.0, rho = 0, unbalanced = FALSE)
# FE (within)
fe_2 <- feols(y ~ x | id, data = df2, cluster = "id")
# FD transform: Δy = y_t - y_{t-1}, Δx likewise (within id sorted by t)
fd2 <- df2 %>%
arrange(id, t) %>%
group_by(id) %>%
mutate(dy = y - dplyr::lag(y), dx = x - dplyr::lag(x)) %>%
ungroup() %>%
filter(!is.na(dy), !is.na(dx))
fd_2 <- feols(dy ~ dx, data = fd2, cluster = "id")
cat("\n=== T = 2, iid errors ===\n")
print(coef(fe_2)["x"])
print(coef(fd_2)["dx"])
cat("Identical beta? ", isTRUE(all.equal(unname(coef(fe_2)["x"]), unname(coef(fd_2)["dx"]), tol = 1e-10)), "\n")
etable(fe_2, fd_2, dict = c(x = "x (FE)", dx = "Δx (FD)"))
# ------------------------------------------------------------
# 2) T = 5, iid errors  -> generally NOT identical, FE often more efficient
# ------------------------------------------------------------
df5 <- sim_panel(N = 1000, T = 5, beta = 1.5, rho = 0, unbalanced = FALSE)
fe_5 <- feols(y ~ x | id, data = df5, cluster = "id")
fd5 <- df5 %>%
arrange(id, t) %>%
group_by(id) %>%
mutate(dy = y - dplyr::lag(y), dx = x - dplyr::lag(x)) %>%
ungroup() %>%
filter(!is.na(dy), !is.na(dx))
fd_5 <- feols(dy ~ dx, data = fd5, cluster = "id")
cat("\n=== T = 5, iid errors ===\n")
print(coef(fe_5)["x"])
print(coef(fd_5)["dx"])
cat("Identical beta? ", isTRUE(all.equal(unname(coef(fe_5)["x"]), unname(coef(fd_5)["dx"]), tol = 1e-10)), "\n")
etable(fe_5, fd_5, dict = c(x = "x (FE)", dx = "Δx (FD)"))
# ------------------------------------------------------------
# 3) T = 5, AR(1) errors  -> transformed error structure differs; SEs can diverge
# ------------------------------------------------------------
df5_ar <- sim_panel(N = 1000, T = 5, beta = 1.5, rho = 0.6, unbalanced = FALSE)
fe_5_ar <- feols(y ~ x | id, data = df5_ar, cluster = "id")
fd5_ar <- df5_ar %>%
arrange(id, t) %>%
group_by(id) %>%
mutate(dy = y - dplyr::lag(y), dx = x - dplyr::lag(x)) %>%
ungroup() %>%
filter(!is.na(dy), !is.na(dx))
fd_5_ar <- feols(dy ~ dx, data = fd5_ar, cluster = "id")
cat("\n=== T = 5, AR(1) errors (rho=0.6) ===\n")
etable(fe_5_ar, fd_5_ar, dict = c(x = "x (FE)", dx = "Δx (FD)"))
# ------------------------------------------------------------
# 4) Unbalanced panel (T = 5 target) -> FE uses all periods; FD loses first differences where gaps exist
# ------------------------------------------------------------
df_unbal <- sim_panel(N = 1000, T = 5, beta = 1.5, rho = 0, unbalanced = TRUE)
fe_unbal <- feols(y ~ x | id, data = df_unbal, cluster = "id")
fd_unbal <- df_unbal %>%
arrange(id, t) %>%
group_by(id) %>%
mutate(dy = y - dplyr::lag(y), dx = x - dplyr::lag(x)) %>%
ungroup() %>%
filter(!is.na(dy), !is.na(dx))
fd_unbal_fit <- feols(dy ~ dx, data = fd_unbal, cluster = "id")
cat("\n=== Unbalanced panel (target T = 5, iid errors) ===\n")
etable(fe_unbal, fd_unbal_fit, dict = c(x = "x (FE)", dx = "Δx (FD)"))
# ------------------------------------------------------------
# Takeaways printed to console
# ------------------------------------------------------------
cat("\nKey takeaways:\n",
"• With T=2 and iid errors: FE and FD slope are numerically identical.\n",
"• With T>2: FE and FD generally differ (different weighting of within variation).\n",
"• With serial correlation (e.g., AR(1)): transformed error structures differ; SEs can change.\n",
"• In unbalanced panels: FD can drop more observations; FE often uses more info.\n")
# We are going to replicate Carpenter and Dobkin on the effects of minimum drinking age on
# mortality - This is the exercise conducted by Philip Leppert.
rm(list = ls()) # clear memory
# Replace this with whatever your directory is.
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(dplyr,ggplot2,rdrobust,magrittr)
# Read in data
carpenter_dobkin_2009 <- read.csv("dobkin.csv")
View(carpenter_dobkin_2009)
View(carpenter_dobkin_2009)
carpenter_dobkin_2009 %>%
ggplot(aes(x = agecell, y = all)) +
geom_point() +
geom_vline(xintercept = 21, color = "#ff0091", linewidth = 1, linetype = "dashed") +
annotate("text", x = 20.4, y = 105, label = "Minimum Drinking Age") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)") +
theme_classic(base_size = 14)
lm_same_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21))
# Output of the model.
summary(lm_same_slope)
carpenter_dobkin_2009 %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
# You can do this in a canned routine. I am replicating what we did above (standard errors are different)
rd_out <- rdrobust(y = carpenter_dobkin_2009$ all, x = carpenter_dobkin_2009$agecell, , h=100, c = 21, p = 1, kernel = "uniform")
summary(rd_out)
lm_different_slope <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + threshold:I(agecell - 21))
summary(lm_different_slope)
# Now quadratic by hand
lm_quadratic <- carpenter_dobkin_2009 %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + I((agecell -21)^2) + threshold:I(agecell - 21) +
threshold:I((agecell - 21)^2))
summary(lm_quadratic)
carpenter_dobkin_2009 %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm",
formula = y ~ x + I(x ^ 2),
se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
# Now let's limit the sample! Only folks between 20 and 22!
lm_sensitivity <- carpenter_dobkin_2009 %>%
filter(agecell >= 20 & agecell <= 22) %>%
mutate(threshold = ifelse(agecell >= 21, 1, 0)) %$%
lm(all ~ threshold + I(agecell - 21) + threshold:I(agecell - 21))
summary(lm_sensitivity)
carpenter_dobkin_2009 %>%
filter(agecell >= 20 & agecell <= 22) %>%
select(agecell, all) %>%
mutate(threshold = as.factor(ifelse(agecell >= 21, 1, 0))) %>%
ggplot(aes(x = agecell, y = all, color = threshold)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
scale_color_brewer(palette = "Accent") +
guides(color = FALSE) +
geom_vline(xintercept = 21, color = "red",
size = 1, linetype = "dashed") +
labs(y = "Mortality rate (per 100.000)",
x = "Age (binned)")+
theme_classic(base_size = 14)
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
set.seed(22092008) # set random number generator seed
library(pacman)
p_load(tidyverse,broom,rdrobust,estimatr,modelsummary)
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
set.seed(22092008) # set random number generator seed
library(pacman)
p_load(tidyverse,broom,rdrobust,estimatr,modelsummary)
# This is an excellent example from Andrew Weiss:
# hypothetical example, students take an entrance exam
# at the beginning of a school year.
# Those who score 70 or below are automatically enrolled
# in a free tutoring program and receive assistance throughout the year.
# At the end of the school year, students take a final test,
# or exit exam (with a maximum of 100 points) to measure how much
# they learned overall.
tutoring <- read_csv("tutoring_program_fuzzy.csv")
View(tutoring)
View(tutoring)
ggplot(tutoring, aes(x = entrance_exam, y = tutoring_text, color = entrance_exam <= 70)) +
# Make points small and semi-transparent since there are lots of them
geom_point(size = 1.5, alpha = 0.5,
position = position_jitter(width = 0, height = 0.25, seed = 1234)) +
# Add vertical line
geom_vline(xintercept = 70) +
# Add labels
labs(x = "Entrance exam score", y = "Participated in tutoring program") +
# Turn off the color legend, since it's redundant
guides(color = "none")
# You can generate a table showing these numbers
tutoring %>%
group_by(tutoring, entrance_exam <= 70) %>%
summarize(count = n()) %>%
group_by(tutoring) %>%
mutate(prop = count / sum(count))
ggplot(tutoring, aes(x = entrance_exam, y = exit_exam, color = tutoring)) +
geom_point(size = 1, alpha = 0.5) +
# Add a line based on a linear model for the people scoring less than 70
geom_smooth(data = filter(tutoring, entrance_exam <= 70), method = "lm") +
# Add a line based on a linear model for the people scoring 70 or more
geom_smooth(data = filter(tutoring, entrance_exam > 70), method = "lm") +
geom_vline(xintercept = 70) +
labs(x = "Entrance exam score", y = "Exit exam score", color = "Used tutoring")
tutoring_with_bins <- tutoring %>%
mutate(exam_binned = cut(entrance_exam, breaks = seq(0, 100, 5))) %>%
# Group by each of the new bins and tutoring status
group_by(exam_binned, tutoring) %>%
# Count how many people are in each test bin + used/didn't use tutoring
summarize(n = n()) %>%
# Make this summarized data wider so that there's a column for tutoring and no tutoring
pivot_wider(names_from = "tutoring", values_from = "n", values_fill = 0) %>%
rename(tutor_yes = `TRUE`, tutor_no = `FALSE`) %>%
# Find the probability of tutoring in each bin by taking
# the count of yes / count of yes + count of no
mutate(prob_tutoring = tutor_yes / (tutor_yes + tutor_no))
ggplot(tutoring_with_bins, aes(x = exam_binned, y = prob_tutoring)) +
geom_col() +
geom_vline(xintercept = 8.5) +
labs(x = "Entrance exam score", y = "Proportion of people participating in program")
# Let's make an instrument and center this puppy
tutoring_centered <- tutoring %>%
mutate(entrance_centered = entrance_exam - 70,
below_cutoff = entrance_exam <= 70)
tutoring_centered
# Now we have a new column named below_cutoff that we’ll use as an instrument. Most of the time this will be the same as the tutoring column, since most people are compliers. But some people didn’t comply, like person 8 here who was not below the cutoff but still used the tutoring program.
# Let's pretend this cutoff is sharp.
model_sans_instrument <- lm(exit_exam ~ entrance_centered + tutoring,
data = filter(tutoring_centered,
entrance_centered >= -10 &
entrance_centered <= 10))
tidy(model_sans_instrument)
model_fuzzy <- iv_robust(
exit_exam ~ entrance_centered + tutoring | entrance_centered + below_cutoff,
data = filter(tutoring_centered, entrance_centered >= -10 & entrance_centered <= 10)
)
tidy(model_fuzzy)
# A little panel data exercise
# Explain variation in life expectancy across countries.
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(ggplot2,lfe)
wb <- read.csv("life.csv")
View(wb)
View(wb)
mod_1 <- felm(Life.expectancy ~ Schooling, data = wb)
# A little panel data exercise
# Explain variation in life expectancy across countries.
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(ggplot2,lfe)
wb <- read.csv("life.csv")
# Start with a simple OLS regression of Life.Expectancy on Schooling
mod_1 <- felm(Life.expectancy ~ Schooling, data = wb)
# A little panel data exercise
# Explain variation in life expectancy across countries.
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(ggplot2,lfe)
# A little panel data exercise
# Explain variation in life expectancy across countries.
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(ggplot2,lfe)
wb <- read.csv("life.csv")
# Start with a simple OLS regression of Life.Expectancy on Schooling
mod_1 <- felm(Life.expectancy ~ Schooling, data = wb)
summary(mod_1)
# A little panel data exercise
# Explain variation in life expectancy across countries.
rm(list = ls()) # clear memory
setwd("/Users/auffhammer/Library/CloudStorage/Dropbox/06_Teaching/MACSS/2025/COMPSS_201_2025/lecture_10")
#Jupyter Path is
#setwd("~/COMPSS_201_2025/lecture_10")
library(pacman)
p_load(ggplot2,lfe)
wb <- read.csv("life.csv")
# Start with a simple OLS regression of Life.Expectancy on Schooling
mod_1 <- felm(Life.expectancy ~ Schooling, data = wb)
summary(mod_1)
# Control for Country Fixed Effects
mod_2 <- felm(Life.expectancy ~ Schooling| Country, data = wb)
summary(mod_2)
View(wb)
View(wb)
# Control for Country & Year Fixed Effects
mod_3 <- felm(Life.expectancy ~ Schooling| Country + Year, data = wb)
summary(mod_3)
mod_4 <- felm(Life.expectancy ~ Schooling + GDP | Country + Year, data = wb)
summary(mod_4)
